# Weekly Progress Report – Ash

**Project:** IDQS Summer '25  
**Week of:** June 19th, 2025 (Week 7)

---

## ✅ Accomplishments

- Developer 1: Begin designing the backend API using the chosen framework (Flask/FastAPI) to handle user input from the browser. (Week 3)
- Developer 1: Focus on LLM integration and prompt engineering PoC. Experiment with different prompt structures to translate natural language questions into structured queries (initially simplified, not necessarily SQL). (Week 2)

## 🔜 Next Steps

## Schema Awareness

- Prompt engineering for LLMs will be crucial for effective data retrieval, and with the database schema in place, we can optimize our queries and responses.

```
schema_context = """
You have access to the following table:
Table: orders
Columns: order_id (int), customer_name (text), total_amount (float), order_date (date)
"""
```
<!-- Maybe do some relevant data retrieval from the database to augment the prompt before sending it to the LLM? -->
<!-- def generate_response(prompt):
    relevant_data = search_db(prompt)
    full_prompt = f"User asked: {prompt}\nRelevant info:\n{relevant_data}\nAnswer:"
    return call_llm(full_prompt) -->


## LLM Integration

## Prompt Engineering

## Connect to Database

<!-- 🧠 Core Steps of a RAG System
Step	Description	Your Implementation So Far
1️⃣ User Input	Prompt/question from user	✅ HTML form → JS → Flask POST
2️⃣ Preprocessing	(Optional) Clean or interpret prompt	⬜︎ Can add later
3️⃣ Retrieve	Pull relevant context/data from DB or vector store	🟡 You’ll add this soon
4️⃣ Augment Prompt	Combine user prompt + retrieved info	⬜︎ Soon
5️⃣ Generate	Feed prompt into LLM to generate answer	🟡 Using a dummy/stub for now
6️⃣ Respond	Return LLM response to user	✅ Working now via /ask -->